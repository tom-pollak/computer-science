{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Practical 6A: Basic neural evolution</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["Simon O'Keefe: simon.okeefe@york.ac.uk\n","\n","Danny Roberts: danny.roberts@york.ac.uk\n","\n","Tianda Sun: tianda.sun@york.ac.uk"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0073e6\">Prerequisites</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before participating in this practical make sure that you have watched the neural network lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0073e6\">Topics</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic neural evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0073e6\">Learning objectives</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To understand how to evolve weights in a static topology neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Practical instructions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical you will evolve weights of a simple MLP. The problem that you will address is the 'exclusive OR' problem (XOR). We are using this problem, because it is simple, yet it is known that a hidden layer is needed in a MPL to solve this provlem because it is not linearly seperable. The XOR problem takes in two binary inputs and outputs a 1 if either are a one, but not both:\n",
    "\n",
    "<img src=\"xor.png\" alt=\"xor\" width=160>\n",
    "\n",
    "Don't worry; you will get to have a go at a more complex game example soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Defining a simple neural network</span>\n",
    "\n",
    "There are packages out there dedicated to producing neural networks, such as Tensorflow and Pytorch. However, for simplicity we will implement our own basic MLP, fully-connected and hard-coded with just one hidden layer. This is implemented the same as in the walkthrough. **However, for speed, this implementation has only one hidden layer.**\n",
    "\n",
    "<img src=\"MLP.jpg\" alt=\"MLP\" width=400>\n",
    "\n",
    "Let's start by defining a class for our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, numInput, numHidden, numOutput):\n",
    "        self.fitness = 0\n",
    "        self.numInput = numInput + 1 # Add bias node to inputs\n",
    "        self.numHidden = numHidden\n",
    "        self.numOutput = numOutput\n",
    "\n",
    "        self.wh = np.random.randn(self.numHidden, self.numInput) \n",
    "        self.wo = np.random.randn(self.numOutput, self.numHidden)\n",
    "\n",
    "        self.ReLU = lambda x : max(0,x)\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        try:\n",
    "            ans = (1 / (1 + math.exp(-x)))\n",
    "        except OverflowError:\n",
    "            ans = float('inf')\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the feedforward function of our network. To do this, we simply take the dot product of the input array and the weights from that input to the next layer of nodes. We then run those weighted sums through the ReLU function in the hidden layer, and the sigmoid in the last layer. This makes it similar to a non-linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(MLP):\n",
    "    def feedForward(self, inputs):\n",
    "        inputsBias = inputs[:]\n",
    "        inputsBias.insert(len(inputs),1)                 # Add bias input\n",
    "        h1 = np.dot(self.wh, inputsBias)                 # feed to hidden layer\n",
    "        h1 = [self.ReLU(x) for x in h1]              # Activate hidden layer\n",
    "        output = np.dot(self.wo, h1)                 # feed to output layer\n",
    "        output = [self.sigmoid(x) for x in output]   # Activate output layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define functions that allow the genetic algorithm to get and set the weights as a simple one-dimensional list. This means we can then just work with the built-in operators without having to worry about defining our own to work with multidimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(MLP):\n",
    "    \n",
    "    def getWeightsLinear(self):\n",
    "        flat_wh = list(self.wh.flatten())\n",
    "        flat_wo = list(self.wo.flatten())\n",
    "        return( flat_wh + flat_wo )\n",
    "\n",
    "    def setWeightsLinear(self, Wgenome):\n",
    "        numWeights_IH = self.numHidden * (self.numInput)\n",
    "        self.wh = np.array(Wgenome[:numWeights_IH])\n",
    "        self.wh = self.wh.reshape((self.numHidden, self.numInput))\n",
    "        self.wo = np.array(Wgenome[numWeights_IH:])\n",
    "        self.wo = self.wo.reshape((self.numOutput, self.numHidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a multi-layer perceptron with 2 inputs, 3 hidden nodes (in a single hidden layer), and 1 output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = MLP(2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = myNet.getWeightsLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes in a list of size 2, and gives a list as output, with each element in the list being the output nodes (here we only have 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = myNet.feedForward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome will be between 0 and 1, due to the sigmoid function. To make this binary we can add a step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02794216910939127]\n"
     ]
    }
   ],
   "source": [
    "print(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(outcome[0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Exercise: Implement your Genetic Algorithm Here</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENVCOenv2020",
   "language": "python",
   "name": "envcoenv2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
