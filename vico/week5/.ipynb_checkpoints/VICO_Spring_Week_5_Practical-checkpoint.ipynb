{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4zPi4-Sn__KB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-contrib-python==4.5.5.62\n",
      "  Downloading opencv_contrib_python-4.5.5.62-cp37-abi3-macosx_11_0_arm64.whl (36.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /Users/tom/.asdf/installs/python/3.11.1/lib/python3.11/site-packages (from opencv-contrib-python==4.5.5.62) (1.24.1)\n",
      "Installing collected packages: opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-4.5.5.62\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/tom/.asdf/installs/python/3.11.1/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Reshimming asdf python...\n"
     ]
    }
   ],
   "source": [
    "# The following might be needed if the SIFT code does not run. In this case, you will also need to restart the runtime\n",
    "# !pip install opencv-contrib-python==4.5.5.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrJyJ-ODnSwL"
   },
   "source": [
    "#VICO Spring Week 5 Practical: Local features\n",
    "\n",
    "As usual, let's start with the required imports; we also need to download some of the images we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2839,
     "status": "ok",
     "timestamp": 1673879535032,
     "user": {
      "displayName": "Claudio Guarnera",
      "userId": "04848003249193713442"
     },
     "user_tz": 0
    },
    "id": "khxaKndRGFbT",
    "outputId": "655c03ca-a450-42d7-9012-f45ab67e3565"
   },
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/s/ev2zkedv72ivk18/scene1.png\n",
    "# !wget https://www.dropbox.com/s/uy8t276qptvlrdg/scene2.png\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUGw5N2G49g0"
   },
   "source": [
    "Let's load the images and visualise them; they are two different views of the same scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 3530,
     "status": "ok",
     "timestamp": 1673879542291,
     "user": {
      "displayName": "Claudio Guarnera",
      "userId": "04848003249193713442"
     },
     "user_tz": 0
    },
    "id": "15oT1sr7Azpa",
    "outputId": "48810420-9a06-4944-9cbd-06a2dca53288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14722ab90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread(\"scene1.png\")\n",
    "\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img2 = cv2.imread(\"scene2.png\")\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig=plt.figure(figsize=(16, 16)) \n",
    "fig.add_subplot(1,2,1) \n",
    "plt.imshow(img1) \n",
    "fig.add_subplot(1,2,2) \n",
    "plt.imshow(img2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g9hq3r1zFjY"
   },
   "source": [
    "Let's detect the SIFT features, create the descriptors and visualise their locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 3612,
     "status": "ok",
     "timestamp": 1673879554761,
     "user": {
      "displayName": "Claudio Guarnera",
      "userId": "04848003249193713442"
     },
     "user_tz": 0
    },
    "id": "9wh5ttY4CVjD",
    "outputId": "360c1b24-fcbb-4036-eef5-f45eca85de76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124d27d00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We could use colour images to detect the features. However, they might be easier to visualise on greyscale images\n",
    "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a SIFT detector and inizialise it with the default parameters. N.B. : the parameter values might differ from the ones suggested in the 2004 paper by Lowe.\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Feature detection\n",
    "key_points1 = sift.detect(img1_gray,None)\n",
    "# Feature description\n",
    "_, descriptors1 = sift.compute(img1_gray, key_points1) \n",
    "imgOut1 = np.copy(img1_gray)\n",
    "imgKp1 = cv2.drawKeypoints(img1_gray,key_points1, imgOut1 ,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# We can also do feature detection and description in one step\n",
    "key_points2, descriptors2 = sift.detectAndCompute(img2_gray,None)\n",
    "#_, descriptors2 = sift.compute(img2_gray, key_points2) \n",
    "imgOut2 = np.copy(img2_gray)\n",
    "imgKp2 = cv2.drawKeypoints(img2_gray,key_points2, imgOut2 ,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig=plt.figure(figsize=(16, 16)) \n",
    "fig.add_subplot(1,2,1) \n",
    "plt.imshow(imgKp1, cmap = \"gray\") \n",
    "fig.add_subplot(1,2,2) \n",
    "plt.imshow(imgKp2, cmap = \"gray\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHwq93Oo4j85"
   },
   "source": [
    "If you look at the features in the two images, there are a few that are clearly matching on the walls, right above the plant and on the right side: they are in corresponding locations and have the same orientation (with respect to the floor). However, their scale - the radius of the circle representing the feature - is different, since the two images are at different scales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbADgdJm9zfb"
   },
   "source": [
    "###Task 1: SIFT features matching\n",
    "\n",
    "Let's try to match the features. We will create a brute-force matcher and get it to find the k nearest neighbors.\n",
    "Once you have found possible matches, try to discard incorrect matches using the criterion defined by Lowe (the threshold *th* is 0.7).\n",
    "\n",
    "Once you have done this, display all the \"correct\" matches. Can you spot any obviously wrong match? If there is too much clutter, use multiple images and try to show only a small subset of matches in a given image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-Wsotl2CeI2"
   },
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv2.BFMatcher.create(cv2.NORM_L2, crossCheck = False)\n",
    "n_neighbours = # Your code goes here\n",
    "matches = bf.knnMatch(descriptors1,descriptors2, n_neighbours)\n",
    "\n",
    "possiblyCorrectMatches = # Your code goes here\n",
    "# Your code goes here: apply the criterion definend by Lowe; th=0.7\n",
    "\n",
    "# Draw the matches and visualise them\n",
    "img_matched = cv2.drawMatchesKnn(img1, key_points1, img2, key_points2, possiblyCorrectMatches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "fig=plt.figure(figsize=(24,24)) \n",
    "plt.imshow(img_matched)\n",
    "plt.show()\n",
    "\n",
    "# Draw only a small subset set of matches at a time and visualise them\n",
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhpOgfsLaf_i"
   },
   "source": [
    "###Task 2: panoramic stitching using SIFT\n",
    "\n",
    "Let's go back to stitching images into a panorama (see Week 4 practical); we will reuse some of the code.\n",
    "However, this time we will use SIFT features to (hopefully!) locate corresponding point pairs. In particular, we will use all the point for which a \"good\" match has been found, according to the same criterion used in Task 1.\n",
    "\n",
    "How do the combined images look like? You could try again, using th = 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrScVWBeaw36"
   },
   "outputs": [],
   "source": [
    "# Download an image pair\n",
    "!wget https://www.dropbox.com/s/zd3j502jll2fwwx/scene_panoA.png\n",
    "!wget https://www.dropbox.com/s/9x990flbwadppg0/scene_panoB.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3h6fIJR71Ay"
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "imageA = cv2.cvtColor(cv2.imread('scene_panoA.png'), cv2.COLOR_BGR2RGB) \n",
    "imageB = cv2.cvtColor(cv2.imread('scene_panoB.png'), cv2.COLOR_BGR2RGB) \n",
    "\n",
    "# Display original images\n",
    "fig=plt.figure(figsize=(16, 16)) \n",
    "fig.add_subplot(1,2,1) \n",
    "plt.imshow(imageA) \n",
    "fig.add_subplot(1,2,2) \n",
    "plt.imshow(imageB) \n",
    "plt.show()\n",
    "\n",
    "# locate the SIFT features and compute the descriptors\n",
    "# Your code goes here\n",
    "\n",
    "bf = cv2.BFMatcher.create(cv2.NORM_L2, crossCheck = False)\n",
    "matches = bf.knnMatch( # Your code goes here )\n",
    "\n",
    "# look for good matches and store the corresponding points for computing the homography\n",
    "targetpts = # Your code goes here\n",
    "sourcepts = # Your code goes here\n",
    "# Your code goes here\n",
    "\n",
    "# Compute best fit homography from all the points in targetpts and sourcepts\n",
    "# Your code goes here\n",
    "\n",
    "# Warp image B and display result\n",
    "result = cv2.warpPerspective(imageB, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "# Combine with image A and display\n",
    "result[0:imageA.shape[0], 0:imageA.shape[1]] = imageA\n",
    "fig=plt.figure(figsize=(24,24)) \n",
    "plt.imshow(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1B4yRDwyorK"
   },
   "source": [
    "###Task 3: RANSAC\n",
    "\n",
    "Lowering the threshold to 0.5 seems to work. However, in this way we have discarded a number of valid matches. Depending on the application, we might be left with not enough points to perform the task. \n",
    "\n",
    "Instead, we should have used RANSAC. \n",
    "\n",
    "Let's say we have 30% outliers among the matched feature points. What would be a suitable value for the maximum number of RANSAC iterations, if we want p=0.99?\n",
    "\n",
    "Implement from scratch the basic version of RANSAC seen in the slides and use it on the matched keypoints from the previous example (matching *th* = 0.7). Use it to find outliers and compute the homography from inliers only. The main body of the algorithm should require 30 lines of code or less.\n",
    "\n",
    "Label a point as inlier if the reprojection error is <= 3 pixel. Given the above, what would be a reasonable value for *T*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXnfXyodynSD"
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "po = 0.30\n",
    "p = 0.99\n",
    "d = 3\n",
    "S = # Your code goes here\n",
    "N = # Your code goes here\n",
    "T = # Your code goes here\n",
    "\n",
    "# store the randomly picked points here\n",
    "tmpTargetpts = # Your code goes here\n",
    "tmpSourcepts = # Your code goes here\n",
    "\n",
    "\n",
    "for iteration in range(N):\n",
    "  # Your code goes here\n",
    "  \n",
    "  \n",
    "print(H)\n",
    "print(H_final)\n",
    "fig=plt.figure(figsize=(24,24)) \n",
    "resultOld = cv2.warpPerspective(imageB, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "resultOld[0:imageA.shape[0], 0:imageA.shape[1]] = imageA\n",
    "plt.imshow(resultOld)\n",
    "plt.show()\n",
    "\n",
    "fig=plt.figure(figsize=(24,24)) \n",
    "resultRANSAC = cv2.warpPerspective(imageB, H_final, (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "resultRANSAC[0:imageA.shape[0], 0:imageA.shape[1]] = imageA\n",
    "plt.imshow(resultRANSAC)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLmX8f21WgbU"
   },
   "source": [
    "###Task 4: DoG Scale-space\n",
    "\n",
    "Load the image and build the first SIFT octave (s = 2, start with $\\sigma = \\sqrt(2)/2$); compute and display the Difference of Gaussian images.\n",
    "Before processing the image, convert it into grayscale and normalise it.\n",
    "You can blur the image using the function `cv2.GaussianBlur`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCqgwxxGWmIR"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/p8rpfe2qr7hw1tj/man.png\n",
    "\n",
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMDdByfSqBqZi0sa6eT5TvJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "computer-science",
   "language": "python",
   "name": "computer-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
